

# üéì Coach Agent

**Coach Agent** es un tutor de ingl√©s inteligente e interactivo construido con **Python** y **Streamlit**. Utiliza Modelos de Lenguaje Locales (LLMs) para mantener conversaciones naturales, corregir gram√°tica en tiempo real y adaptar el curr√≠culum (A1-C2) bas√°ndose en los intereses y objetivos del estudiante.

-----

## ‚ú® Caracter√≠sticas Principales

  * **üß† IA Local y Privada:** Ejecuta modelos LLM (Llama/Mistral) y correcci√≥n gramatical (T5) localmente. ¬°No requiere API keys de pago\!
  * **üó£Ô∏è Conversaci√≥n Natural:** Chat fluido con memoria de contexto.
  * **üîß Corrector Gramatical en Tiempo Real:** Detecta errores antes de que la IA responda y sugiere correcciones amigables.
  * **üìà Sistema de Progreso Din√°mico:** Curr√≠culum alineado al MCER (A1 a C2) que avanza autom√°ticamente cuando el estudiante domina un tema.
  * **üéß Experiencia Auditiva (TTS):** Reproducci√≥n autom√°tica de audio para practicar *listening* y pronunciaci√≥n.
  * **üë§ Perfil Inteligente (Auto-Learn):** El sistema aprende tu nombre, hobbies y objetivos a trav√©s de la conversaci√≥n natural ("Onboarding Invisible").
  * **üé® UI/UX Moderna:** Interfaz limpia con feedback visual no intrusivo (`toasts`), modo oscuro y m√©tricas de progreso.

-----

## üìÇ Estructura del Proyecto

```text
ai-coach/
‚îú‚îÄ‚îÄ main.py                  # üöÄ Punto de entrada (Interfaz Streamlit)
‚îú‚îÄ‚îÄ config.py                # ‚öôÔ∏è Configuraci√≥n (Curriculum, T√≠tulos)
‚îú‚îÄ‚îÄ requirements.txt         # üì¶ Dependencias de Python
‚îú‚îÄ‚îÄ models/                  # üß† Carpeta para modelos (.gguf, .bin)
‚îÇ   ‚îú‚îÄ‚îÄ chat_model.gguf      # Modelo principal (Llama-3/Mistral) Debe descargarse
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ coach.py             # üß† L√≥gica pedag√≥gica (Estado, Onboarding, Reglas)
‚îÇ   ‚îî‚îÄ‚îÄ engine.py            # ‚öôÔ∏è Carga de modelos (Torch, LlamaCPP)
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ audio.py             # üéß Generaci√≥n y renderizado de audio (TTS)
‚îÇ   ‚îî‚îÄ‚îÄ data.py              # üíæ Manejo de JSON (Guardar/Cargar)
‚îú‚îÄ‚îÄ data/ (Auto-generado)
‚îÇ   ‚îú‚îÄ‚îÄ student_profile.json # Datos del usuario
‚îÇ   ‚îî‚îÄ‚îÄ student_progress.json# Progreso del curso
‚îî‚îÄ‚îÄ docs/                    # üìÑ Documentaci√≥n adicional
```

-----

## üöÄ Instalaci√≥n y Uso

### 1\. Requisitos Previos

  * Python 3.10+
  * RAM recomendada: 8GB+ (para ejecutar modelos cuantizados).

### 2\. Instalaci√≥n

Clona el repositorio e instala las dependencias:

```bash
git clone https://github.com/raalzate/ia-coach-agent.git
cd ia-coach-agent
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### 3\. Configuraci√≥n de Modelos

Debes descargar los modelos y colocarlos en la carpeta `models/`:

1.  **Chat Model:** Descarga un modelo `.gguf` (ej. *Llama-3-8B-Instruct-Q4\_K\_M.gguf*) y gu√°rdalo en `models/`.
2.  **Grammar Model:** Descarga *vennify/t5-base-grammar-correction* (o similar) desde HuggingFace.

### 4\. Ejecuci√≥n

```bash
streamlit run main.py
```

-----

## üß† Arquitectura T√©cnica

### Flujo de Interacci√≥n (The Interaction Loop)

1.  **Input:** El usuario escribe en el chat.
2.  **Interceptor:**
      * Comandos (`/learn`): Ejecuta acciones de sistema.
      * Auto-Learn: Extrae entidades (Nombre, Edad, Hobbies) silenciosamente.
3.  **Grammar Guardrail (T5):**
      * El input pasa por un modelo T5 especializado.
      * Si hay error -\> Se muestra una alerta suave (Toast + Warning) y se pide reintentar.
      * Si es correcto -\> Pasa al LLM.
4.  **Coach Brain (LLM):**
      * Recibe un *System Prompt* din√°mico inyectado con el perfil del usuario y el tema actual.
      * Genera respuesta v√≠a *Streaming* (token a token).
5.  **Audio (TTS):**
      * Se genera audio en segundo plano.
      * Se reproduce con un `autoplay` invisible para fluidez inmediata.

### Gesti√≥n de Estado (Session State)

El sistema utiliza persistencia h√≠brida:

  * **Session State (RAM):** Para el historial de chat inmediato y objetos de modelos cargados.
  * **JSON (Disco):** Para persistencia a largo plazo (Progreso del estudiante y Perfil).

-----

## ‚öôÔ∏è Personalizaci√≥n (Config.py)

Puedes ajustar el plan de estudios en `config.py`. El sistema se adapta autom√°ticamente a nuevos niveles.

```python
CURRICULUM = {
    "Level 1 (Beginner)": [
        "Present Simple", 
        "Verb To Be",
        "Family & Hobbies"
    ],
    "Level 2 (Elementary)": [
        "Past Simple",
        "Future Plans"
    ]
    # ... Agrega niveles libremente
}
```

-----

## üé® Gu√≠a de Estilos UI/UX

La interfaz utiliza hacks de CSS inyectados para mejorar la experiencia nativa de Streamlit:

  * **Toasts (`st.toast`):** Usados para notificaciones de √©xito ("Memory Updated") o avisos r√°pidos ("Checking grammar...").
  * **Avatares:**
      * üë©‚Äçüè´ **Emma (Coach):** Gu√≠a principal.
      * üßê **Corrector:** Aparece solo cuando hay errores gramaticales.
  * **Sidebar:** Act√∫a como el "Cuaderno del Estudiante", mostrando m√©tricas claras y barra de progreso verde (`#4CAF50`).

-----

# üì• Gu√≠a de Descarga Manual: Modelo Llama 3 (GGUF)

Esta gu√≠a explica c√≥mo obtener el archivo `.gguf` necesario para ejecutar el **AI English Coach**. Estamos buscando espec√≠ficamente la versi√≥n `Q4_K_M` (4-bit Medium Quantization), que ofrece el mejor equilibrio entre velocidad, uso de memoria (aprox. 5-6 GB de RAM) e inteligencia.

## üìã Informaci√≥n del Archivo

  * **Modelo Base:** Meta Llama 3 8B Instruct.
  * **Formato:** GGUF (Optimizado para CPU/Apple Silicon).
  * **Cuantizaci√≥n:** `Q4_K_M` (Recomendado).
  * **Tama√±o:** Aproximadamente **4.9 GB**.
  * **Fuente:** Hugging Face (Repositorios de la comunidad como *Bartowski* o *MaziyarPanahi*).

-----

## üöÄ Opci√≥n 1: Descarga Directa desde el Navegador (M√°s F√°cil)

Esta es la forma m√°s sencilla si est√°s en tu ordenador personal.

1.  **Ir al Repositorio:**
    El modelo con las correcciones de tokenizador m√°s estables actualmente es mantenido por **Bartowski**. Accede al siguiente enlace:
    üîó **[Bartowski / Meta-Llama-3-8B-Instruct-GGUF en Hugging Face](https://huggingface.co/bartowski/Meta-Llama-3-8B-Instruct-GGUF/tree/main)**

2.  **Localizar el Archivo:**
    En la lista de archivos, busca el siguiente nombre (o uno muy similar):
    `Meta-Llama-3-8B-Instruct-Q4_K_M.gguf`

    > **Nota:** Es posible que el nombre exacto y largo (`...correct-pre-tokenizer...`) haya sido simplificado en las versiones m√°s nuevas porque la correcci√≥n ya es est√°ndar. El archivo `Q4_K_M.gguf` de Bartowski ya incluye estas correcciones.

3.  **Descargar:**
    Haz clic en el icono de descarga (‚¨áÔ∏è) situado a la derecha del nombre del archivo.

4.  **Ubicaci√≥n en el Proyecto:**
    Una vez descargado, mueve el archivo a la carpeta `models/` dentro de tu proyecto.

    ```text
    ai-coach/
    ‚îú‚îÄ‚îÄ main.py
    ‚îú‚îÄ‚îÄ models/
    ‚îÇ   ‚îî‚îÄ‚îÄ Meta-Llama-3-8B-Instruct-Q4_K_M.gguf  <-- ¬°AQU√ç!
    ```

-----

## üíª Opci√≥n 2: Descarga v√≠a Terminal (Recomendado para Servidores)

Si est√°s en un entorno Linux/Mac o prefieres la terminal, usa `wget` para descargar el archivo directamente a la carpeta correcta.

1.  **Navega a la carpeta de modelos:**

    ```bash
    cd ruta/a/tu/proyecto/ai-coach
    mkdir -p models
    cd models
    ```

2.  **Ejecuta el comando de descarga:**
    Este enlace descarga la versi√≥n `Q4_K_M` del repositorio de Bartowski:

    ```bash
    wget https://huggingface.co/bartowski/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf
    ```

    *Si no tienes `wget`, puedes usar `curl`:*

    ```bash
    curl -L -O https://huggingface.co/bartowski/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf
    ```

-----

## ‚öôÔ∏è Configuraci√≥n Final

Una vez tengas el archivo descargado, debes asegurarte de que tu archivo `config.py` apunte al nombre correcto.

1.  Abre `config.py`.
2.  Busca la variable `MODEL_PATH`.
3.  Actual√≠zala con el nombre exacto del archivo que descargaste.

<!-- end list -->

```python
# config.py
import os

# Aseg√∫rate de que coincida con el nombre del archivo descargado
MODEL_NAME = "Meta-Llama-3-8B-Instruct-Q4_K_M.gguf" 

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
MODEL_PATH = os.path.join(BASE_DIR, "models", MODEL_NAME)
```

-----

## üìö Referencias T√©cnicas del Modelo

### ¬øPor qu√© este modelo espec√≠fico?

1.  **Meta Llama 3:**

      * *AI at Meta (2024).* Lanzado en abril de 2024, Llama 3 super√≥ a muchos modelos de c√≥digo abierto anteriores en capacidades de razonamiento y seguimiento de instrucciones.
      * [Blog Oficial de Meta Llama 3](https://ai.meta.com/blog/meta-llama-3/)

2.  **El problema del "Correct Pre-tokenizer & EOS":**

      * Cuando Llama 3 sali√≥, hubo un problema con c√≥mo los archivos GGUF manejaban el token de "Fin de Secuencia" (EOS) y el tokenizador previo. Esto causaba que el modelo a veces no dejara de hablar o generara texto basura al final.
      * Los repositorios modernos (como Bartowski o QuantFactory) ya han parcheado esto. La referencia que ten√≠as en el nombre del archivo alude a este parche cr√≠tico.

3.  **Formato GGUF & Quantization (Q4\_K\_M):**

      * **GGUF:** Es el formato est√°ndar actual para inferencia en CPU con `llama.cpp`. Permite mapeo de memoria eficiente (mmap), lo que significa que el modelo carga casi al instante.
      * **Q4\_K\_M:** Utiliza el m√©todo de "k-quants". Reduce la precisi√≥n de los pesos de 16 bits a 4 bits.
          * *Dettmers, T., et al. (2022).* "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale". (Base te√≥rica de la cuantizaci√≥n).
          * La p√©rdida de "inteligencia" (perplejidad) en Q4\_K\_M es insignificante comparada con la ganancia de velocidad y reducci√≥n de RAM (de 16GB a 5GB).

## ü§ù Contribuci√≥n

1.  Haz un Fork del proyecto.
2.  Crea tu rama de funcionalidad (`git checkout -b feature/AmazingFeature`).
3.  Haz Commit de tus cambios (`git commit -m 'Add some AmazingFeature'`).
4.  Push a la rama (`git push origin feature/AmazingFeature`).
5.  Abre un Pull Request.

-----

## üìÑ Licencia

Este proyecto est√° bajo la Licencia MIT - mira el archivo `LICENSE.md` para detalles.