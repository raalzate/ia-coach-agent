# 锔 AI Engine Documentation

Este m贸dulo es responsable de la **carga, inicializaci贸n y gesti贸n de memoria** de los modelos de Inteligencia Artificial. Utiliza el sistema de cach茅 de Streamlit para asegurar que los modelos pesados se carguen solo una vez al iniciar la aplicaci贸n, y no con cada interacci贸n del usuario.

##  Descripci贸n de Funciones

### 1. `load_grammar_engine()`
Carga el modelo especializado en Correcci贸n de Errores Gramaticales (GEC).

* **Arquitectura:** T5 (Text-to-Text Transfer Transformer).
* **Modelo Espec铆fico:** `vennify/t5-base-grammar-correction`.
* **Biblioteca:** Hugging Face `transformers`.
* **Configuraci贸n Cr铆tica:**
    * `use_fast=False`: Se fuerza el uso del tokenizador lento (basado en Python/SentencePiece) en lugar del r谩pido (Rust) para evitar problemas de compatibilidad conocidos en sistemas macOS con ciertas versiones de `transformers`.
* **Retorno:** Tupla `(tokenizer, model)`.

### 2. `load_chat_engine()`
Carga el Gran Modelo de Lenguaje (LLM) cuantizado para la conversaci贸n general.

* **Arquitectura:** Llama / Mistral (dependiendo del archivo `.gguf`).
* **Biblioteca:** `llama-cpp-python` (Binding de Python para `llama.cpp`).
* **Configuraci贸n Cr铆tica:**
    * `n_ctx=4096`: Define la ventana de contexto (memoria a corto plazo de la conversaci贸n).
    * `n_gpu_layers=0`: Fuerza la ejecuci贸n en **CPU**. Esto garantiza estabilidad m谩xima y compatibilidad universal, evitando errores de VRAM o drivers de CUDA/Metal, aunque sacrifica velocidad de generaci贸n.
* **Retorno:** Instancia del objeto `Llama`.

---

##  Mecanismo de Cach茅 (Performance)

El uso del decorador `@st.cache_resource` es fundamental para la experiencia de usuario.



**Sin cach茅:**
1. Usuario escribe "Hola".
2. Script se reinicia.
3. Carga modelo (5GB) -> Tarda 10 segundos.
4. Genera respuesta.

**Con cach茅 (Implementado):**
1. Usuario escribe "Hola".
2. Script se reinicia.
3. Streamlit detecta que `load_chat_engine` ya se ejecut贸.
4. Recupera el modelo de la RAM (0 segundos).
5. Genera respuesta.

---

#  Referencias y Bibliograf铆a Extensa

A continuaci贸n, se presenta una lista detallada de los modelos, arquitecturas y tecnolog铆as base que componen este motor de IA.

## 1. Motor de Gram谩tica (T5 & Vennify)

El corrector gramatical se basa en la arquitectura **Transformer Encoder-Decoder**.

* **Modelo Implementado:**
    * *Vennify T5 Base Grammar Correction*. (2021). Hugging Face Model Hub. Disponible en: [https://huggingface.co/vennify/t5-base-grammar-correction](https://huggingface.co/vennify/t5-base-grammar-correction). (Un fine-tune espec铆fico sobre el dataset C4_200M).

* **Arquitectura Base (T5):**
    * Raffel, C., et al. (2020). **"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"**. *Journal of Machine Learning Research*. Esta es la publicaci贸n seminal que introdujo T5, proponiendo que cualquier tarea de NLP (traducci贸n, resumen, correcci贸n) puede plantearse como un problema de "texto a texto".
    * [Leer Paper en ArXiv](https://arxiv.org/abs/1910.10683)

* **Tecnolog铆a Subyacente:**
    * Vaswani, A., et al. (2017). **"Attention Is All You Need"**. *Advances in Neural Information Processing Systems*. El paper que invent贸 la arquitectura Transformer, base de T5 y GPT.
    * [Leer Paper en ArXiv](https://arxiv.org/abs/1706.03762)

## 2. Motor de Chat (Llama & GGUF)

El motor de chat utiliza modelos **Decoder-only** optimizados para ejecuci贸n local mediante cuantizaci贸n.

* **Software de Inferencia:**
    * Gerganov, G. (2023). **llama.cpp**: Port of Facebook's LLaMA model in C/C++. Esta librer铆a permite ejecutar LLMs en hardware de consumo (MacBooks, CPUs Intel) eficientemente.
    * Repositorio: [https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp)

* **Formato de Archivo (GGUF):**
    * **GGUF (GPT-Generated Unified Format)**. Introducido en agosto de 2023 por el equipo de `llama.cpp` para reemplazar a GGML. Es un formato binario optimizado para mapeo r谩pido en memoria y soporte de metadatos extensibles.

* **Arquitectura Base (Llama):**
    * **Llama 3 (2024):** AI at Meta. "The Llama 3 Herd of Models". El estado del arte actual en modelos abiertos.
        * [Leer Paper](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/)
    * **Llama 2 (2023):** Touvron, H., et al. "Llama 2: Open Foundation and Fine-Tuned Chat Models".
        * [Leer Paper en ArXiv](https://arxiv.org/abs/2307.09288)
    * **Mistral 7B (Alternativa com煤n):** Jiang, A., et al. (2023). "Mistral 7B". Introduce *Sliding Window Attention* para mayor eficiencia.
        * [Leer Paper en ArXiv](https://arxiv.org/abs/2310.06825)

## 3. Librer铆as de Python

* **Streamlit:**
    * Framework de c贸digo abierto para aplicaciones de Machine Learning. Su mecanismo de `st.session_state` y `st.cache_resource` maneja la reactividad de la aplicaci贸n.
    * Documentaci贸n: [https://docs.streamlit.io/](https://docs.streamlit.io/)

* **Hugging Face Transformers:**
    * Wolf, T., et al. (2020). **"Transformers: State-of-the-Art Natural Language Processing"**. EMNLP 2020.
    * [Leer Paper](https://www.aclweb.org/anthology/2020.emnlp-demos.6/)